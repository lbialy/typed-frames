package org.virtuslab.typedframes

package object internal {
  import scala.language.experimental.macros
  import scala.reflect.macros.blackbox
  import scala.annotation.nowarn

  /**
    * Those are Blackbox macros. This means the types generated by them are known before the macro is expanded.
    * Safer, more sane. Definitely less magical and more IDE-friendly.
    *
    * @param c: blackbox.Context - this context object is provided by the Scala compiler.
    */
  private[typedframes] class ColumnsBlackboxMacros(val c: blackbox.Context) {
    import c.universe._

    /**
      * Decompose lambda passed to function calling this macro - the only lambda that will work is a lambda
      * that accesses a field on the single argument of passed function (vd.name == idt.name). All we are
      * doing is taking the fieldName and returning it.
      *
      * It's a compile-time reflection based on parsing lambda's AST!
      *
      * @param selector: c.Tree - a scala AST of a lambda function
      * @return c.Tree - a scala AST of literal String instance
      */
    def colNameMacroImpl(selector: c.Tree): c.Tree =
      extractPath(selector, Vector.empty) match {
        case Left(error) => c.abort(c.enclosingPosition, error)
        case Right(path) => path
      }

    /**
      * Generates a list of names of fields defined in case class A. Method uses compile-time reflection
      * by asking the compiler to provide a c.WeakTypeTag instance for type A.
      *
      * @tparam A - a type designating your case class (a tuple would also work but why would you do that?)
      * @return List[String] - list of names of all fields in case class in declaration order
      */
    def allColNameMacroImpl[A <: Product: c.WeakTypeTag]: c.Tree = {
      val fieldNames: List[String] = allFieldsOfCaseClass[A]

      q"$fieldNames"
    }

    /**
      * An org.apache.spark.sql.ColumnName version of [[allColNameMacroImpl]].
      *
      * @tparam A - a type designating your case class
      * @return List[ColumnName] - list of names of all fields in case class in declaration order wrapped in ColumnName
      */
    def allColNameColumnNameMacroImpl[A <: Product: c.WeakTypeTag]: c.Tree = {
      val fieldNames: List[String] = allFieldsOfCaseClass[A]

      q"$fieldNames.map(fn => new _root_.org.apache.spark.sql.ColumnName(fn))"
    }

    /**
      * An org.apache.spark.sql.ColumnName version of [[colNameMacroImpl]].
      *
      * @param selector: c.Tree - a scala AST of a lambda function
      * @return c.Tree - a scala AST of a call to [[org.apache.spark.sql.ColumnName]] constructor with String value
      *         of field name as argument
      */
    def colNameColumnNameMacroImpl(selector: c.Tree): c.Tree =
      extractPath(selector, Vector.empty) match {
        case Left(error) => c.abort(c.enclosingPosition, error)
        case Right(path) => q"new _root_.org.apache.spark.sql.ColumnName($path)"
      }

    /**
      * A version of [[colNameMacroImpl]] that allows for proper DataFrame column lineage tracking for use in
      * scenarios of joins between tables that contain columns with the same name.
      *
      * @param selector: c.Tree - a scala AST of a lambda function
      * @param df - c.Tree - a scala AST representing an instance of Spark's Dataset[?] - we don't care about
      *           type parameter used to parameterize Dataset type, we only call [[org.apache.spark.sql.Dataset.col]]
      *           method on it.
      * @return c.Tree - a scala AST of a [[org.apache.spark.sql.Column]] returned by .col method
      */
    def colNameDataFrameTwoArgMacroImpl(selector: c.Tree, df: c.Tree): c.Tree =
      extractPath(selector, Vector.empty) match {
        case Left(error) => c.abort(c.enclosingPosition, error)
        case Right(path) => q"$df.col($path)"
      }

    private def allFieldsOfCaseClass[A: WeakTypeTag] =
      implicitly[c.WeakTypeTag[A]].tpe.members.sorted.collect {
        case m: MethodSymbol if m.isCaseAccessor => m.name.toString
      }

    @nowarn("cat=unused")
    @scala.annotation.tailrec
    private def extractPath(in: Tree, out: Vector[String]): Either[String, Tree] = in match {
      case Function(_, expr) => extractPath(expr, out) // drop (argName) =>
      case Select(expr, TermName(field)) => extractPath(expr, field +: out) // extract .field
      case Ident(TermName(_)) => Right(q"${out.mkString(".")}") // drop argName from before .field
      case _ => Left(s"Path ${showRaw(in)} is not in format _.field1.field2")
    }

  }

}
